# LLM Fine-tuning Pipeline with LoRA/QLoRA

A comprehensive, production-ready pipeline for fine-tuning Large Language Models using Parameter-Efficient Fine-Tuning (PEFT) techniques with MLflow experiment tracking and a user-friendly GUI.

## ğŸš€ Features

- **Parameter-Efficient Fine-tuning**: Support for LoRA and QLoRA techniques
- **Multiple Model Support**: RoBERTa, Llama 2, Mistral, GPT models, and more
- **Experiment Tracking**: Full MLflow integration for reproducible experiments
- **Flexible Configuration**: YAML-based configuration system for easy customization
- **Data Pipeline**: Robust data loading, preprocessing, and validation
- **Model Evaluation**: Comprehensive metrics and custom evaluation functions
- **Web GUI**: Simple interface for model selection and training configuration
- **Production Ready**: Docker support, CI/CD pipelines, and deployment scripts
- **Resume-Friendly**: Industry best practices and professional documentation

## ğŸ“Š Supported Models

| Model Type | Model Names | Use Cases |
|------------|-------------|-----------|
| **Encoder Models** | RoBERTa, BERT, DistilBERT | Classification, NER, QA |
| **Decoder Models** | Llama 2, Mistral, Falcon | Text generation, Chat |
| **Encoder-Decoder** | T5, BART, Flan-T5 | Summarization, Translation |

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Pipeline â”‚    â”‚  Training Core  â”‚    â”‚   Evaluation    â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Data Loading  â”‚â”€â”€â”€â–¶â”‚ â€¢ LoRA/QLoRA    â”‚â”€â”€â”€â–¶â”‚ â€¢ Metrics       â”‚
â”‚ â€¢ Preprocessing â”‚    â”‚ â€¢ MLflow Track. â”‚    â”‚ â€¢ Benchmarking  â”‚
â”‚ â€¢ Validation    â”‚    â”‚ â€¢ Model Manag.  â”‚    â”‚ â€¢ Reporting     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web GUI       â”‚    â”‚  Configuration  â”‚    â”‚   Deployment    â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Model Select  â”‚    â”‚ â€¢ YAML Configs  â”‚    â”‚ â€¢ Docker        â”‚
â”‚ â€¢ Training UI   â”‚    â”‚ â€¢ Hyperparams   â”‚    â”‚ â€¢ API Server    â”‚
â”‚ â€¢ Monitoring    â”‚    â”‚ â€¢ Data Splits   â”‚    â”‚ â€¢ Model Serving â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Quick Start

### Prerequisites

- Python 3.8+
- CUDA-capable GPU (optional but recommended)
- Docker (optional, for containerized deployment)

### Installation

1. **Clone the repository:**
```bash
git clone https://github.com/your-username/llm-finetuning-pipeline.git
cd llm-finetuning-pipeline
```

2. **Create virtual environment:**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies:**
```bash
pip install -r requirements.txt
```

4. **Set up MLflow:**
```bash
# Start MLflow server
mlflow server --host 0.0.0.0 --port 5000
```

### Example: Fine-tune RoBERTa for Sentiment Analysis

```python
from pipeline import FineTuningPipeline
from configs import load_config

# Load configuration
config = load_config("configs/roberta_sentiment.yaml")

# Initialize pipeline
pipeline = FineTuningPipeline(config)

# Run training
results = pipeline.run()

print(f"Training completed! Best F1 Score: {results['best_f1']:.4f}")
```

## ğŸ“ Project Structure

```
llm-finetuning-pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ .env.example
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”‚
â”œâ”€â”€ configs/                    # Configuration files
â”‚   â”œâ”€â”€ models/                # Model-specific configs
â”‚   â”œâ”€â”€ datasets/              # Dataset-specific configs
â”‚   â””â”€â”€ experiments/           # Experiment configs
â”‚
â”œâ”€â”€ src/                       # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/                  # Core pipeline components
â”‚   â”œâ”€â”€ models/                # Model implementations
â”‚   â”œâ”€â”€ data/                  # Data processing
â”‚   â”œâ”€â”€ training/              # Training logic
â”‚   â”œâ”€â”€ evaluation/            # Evaluation metrics
â”‚   â””â”€â”€ utils/                 # Utility functions
â”‚
â”œâ”€â”€ gui/                       # Web interface
â”‚   â”œâ”€â”€ app.py                # Main Flask/Streamlit app
â”‚   â”œâ”€â”€ components/           # UI components
â”‚   â””â”€â”€ static/               # Static assets
â”‚
â”œâ”€â”€ notebooks/                 # Jupyter notebooks
â”‚   â”œâ”€â”€ examples/             # Usage examples
â”‚   â””â”€â”€ experiments/          # Research notebooks
â”‚
â”œâ”€â”€ tests/                     # Unit tests
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ e2e/
â”‚
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ api/                  # API documentation
â”‚   â”œâ”€â”€ guides/               # User guides
â”‚   â””â”€â”€ best_practices/       # Best practices
â”‚
â”œâ”€â”€ scripts/                   # Utility scripts
â”‚   â”œâ”€â”€ setup_env.sh
â”‚   â”œâ”€â”€ download_models.py
â”‚   â””â”€â”€ benchmark.py
â”‚
â””â”€â”€ deployments/              # Deployment configurations
    â”œâ”€â”€ kubernetes/
    â”œâ”€â”€ docker/
    â””â”€â”€ cloud/
```

## ğŸ”§ Configuration

The pipeline uses YAML configuration files for maximum flexibility:

```yaml
# configs/roberta_sentiment.yaml
model:
  name: "roberta-base"
  task_type: "sequence_classification"
  num_labels: 2
  
lora:
  r: 8
  alpha: 16
  dropout: 0.1
  target_modules: ["query", "value"]
  
dataset:
  name: "imdb"
  train_split: "train"
  test_split: "test"
  max_length: 512
  
training:
  epochs: 3
  batch_size: 16
  learning_rate: 2e-5
  warmup_steps: 100
  
mlflow:
  experiment_name: "roberta_sentiment_analysis"
  tracking_uri: "http://localhost:5000"
```

## ğŸ“ˆ Experiment Tracking

All experiments are automatically tracked with MLflow:

- **Parameters**: Model configs, hyperparameters, data splits
- **Metrics**: Training/validation loss, accuracy, F1-score, custom metrics
- **Artifacts**: Model checkpoints, training plots, evaluation reports
- **Models**: Versioned model registry with staging/production promotion

## ğŸ¯ Best Practices Implemented

### LoRA/QLoRA Optimization
- **Memory Efficient**: QLoRA with 4-bit quantization for large models
- **Optimal Parameters**: Research-backed default values for rank, alpha, dropout
- **Target Module Selection**: Automatic identification of optimal layers
- **Mixed Precision**: BF16/FP16 training for faster convergence

### Production Readiness
- **Type Safety**: Full type hints and Pydantic validation
- **Error Handling**: Comprehensive exception handling and logging
- **Testing**: Unit, integration, and end-to-end tests
- **Documentation**: Detailed API docs and user guides
- **Monitoring**: Real-time training metrics and alerts

### Data Management
- **Validation**: Automatic data quality checks and statistics
- **Caching**: Efficient data loading with HuggingFace datasets
- **Preprocessing**: Configurable tokenization and augmentation
- **Streaming**: Support for large datasets that don't fit in memory

## ğŸš€ Deployment Options

### Local Development
```bash
python -m src.gui.app  # Start web interface
python -m src.train configs/roberta_sentiment.yaml  # CLI training
```

### Docker
```bash
docker-compose up  # Full stack with MLflow and GUI
```

### Cloud Deployment
- **AWS SageMaker**: Pre-configured training jobs
- **Google Cloud AI Platform**: Vertex AI integration
- **Azure ML**: Azure Machine Learning pipelines
- **Kubernetes**: Scalable distributed training

## ğŸ“š Examples

- **Text Classification**: [RoBERTa Sentiment Analysis](examples/roberta_sentiment.md)
- **Named Entity Recognition**: [BERT NER](examples/bert_ner.md)
- **Text Generation**: [Llama 2 Chat Fine-tuning](examples/llama2_chat.md)
- **Question Answering**: [T5 QA](examples/t5_qa.md)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Hugging Face](https://huggingface.co/) for transformers and PEFT libraries
- [MLflow](https://mlflow.org/) for experiment tracking
- [LoRA Paper](https://arxiv.org/abs/2106.09685) by Hu et al.
- [QLoRA Paper](https://arxiv.org/abs/2305.14314) by Dettmers et al.

## ğŸ“ Support

- ğŸ“§ Email: support@example.com
- ğŸ’¬ Discord: [Join our community](https://discord.gg/example)
- ğŸ“– Documentation: [docs.example.com](https://docs.example.com)
- ğŸ› Issues: [GitHub Issues](https://github.com/your-username/llm-finetuning-pipeline/issues)

---

â­ **Star this repository if you find it useful!** â­