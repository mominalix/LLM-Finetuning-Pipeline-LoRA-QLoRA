# RoBERTa Base Model Configuration
# Example configuration for fine-tuning RoBERTa for text classification

model:
  name: "roberta-base"
  task_type: "sequence_classification"
  num_labels: 2
  max_length: 512
  use_fast_tokenizer: true
  trust_remote_code: false
  torch_dtype: "auto"
  device_map: "auto"
  low_cpu_mem_usage: true

# LoRA Configuration for RoBERTa
use_lora: true
lora:
  r: 8
  alpha: 16
  dropout: 0.1
  bias: "none"
  target_modules: ["query", "value"]  # RoBERTa uses 'query' and 'value' in attention
  use_rslora: false
  use_dora: false

# QLoRA is typically not needed for RoBERTa base (355M params)
use_qlora: false

# Training configuration optimized for RoBERTa
training:
  output_dir: "./outputs/roberta_base"
  num_train_epochs: 3
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 1
  
  # Optimization
  learning_rate: 2e-5
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Scheduling
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  
  # Evaluation
  evaluation_strategy: "epoch"
  save_strategy: "epoch"
  load_best_model_at_end: true
  metric_for_best_model: "eval_f1"
  save_total_limit: 2
  
  # Performance
  bf16: true  # Use bfloat16 for better performance on modern GPUs
  gradient_checkpointing: true
  dataloader_pin_memory: true
  dataloader_num_workers: 4
  
  # Logging
  logging_strategy: "steps"
  logging_steps: 50
  log_level: "info"

# MLflow configuration
mlflow:
  tracking_uri: "${MLFLOW_TRACKING_URI:http://localhost:5000}"
  experiment_name: "roberta_base_experiments"
  log_model: true
  log_predictions: true
  register_model: false

# Evaluation settings
evaluation:
  compute_metrics: true
  metrics: ["accuracy", "f1", "precision", "recall"]
  save_predictions: true
  prediction_output_dir: "./predictions/roberta_base"

# Environment
seed: 42