# Llama 2 7B Model Configuration with QLoRA
# Example configuration for fine-tuning Llama 2 7B for text classification

model:
  name: "meta-llama/Llama-2-7b-hf"
  task_type: "sequence_classification"
  num_labels: 2
  max_length: 2048
  use_fast_tokenizer: true
  trust_remote_code: false
  torch_dtype: "bfloat16"
  device_map: "auto"
  low_cpu_mem_usage: true

# Use QLoRA for memory-efficient fine-tuning of large models
use_lora: true
use_qlora: true

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  bias: "none"
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]  # Llama 2 attention modules
  use_rslora: false
  use_dora: false

qlora:
  load_in_4bit: true
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true
  bnb_4bit_quant_storage: "uint8"

# Training configuration optimized for large models
training:
  output_dir: "./outputs/llama2_7b"
  num_train_epochs: 2  # Fewer epochs for large models
  per_device_train_batch_size: 2  # Smaller batch size due to memory constraints
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 8  # Larger accumulation for effective batch size
  
  # Optimization
  learning_rate: 1e-4  # Slightly higher LR for LoRA
  weight_decay: 0.05
  max_grad_norm: 0.3
  
  # Scheduling
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  
  # Evaluation
  evaluation_strategy: "steps"
  eval_steps: 100
  save_strategy: "steps"
  save_steps: 500
  load_best_model_at_end: true
  metric_for_best_model: "eval_f1"
  save_total_limit: 1  # Keep fewer checkpoints to save space
  
  # Performance optimizations for large models
  bf16: true
  gradient_checkpointing: true
  dataloader_pin_memory: false  # Disable for large models to save memory
  dataloader_num_workers: 0
  
  # Logging
  logging_strategy: "steps"
  logging_steps: 25
  log_level: "info"
  
  # Early stopping for large model training
  early_stopping_patience: 3
  early_stopping_threshold: 0.001

# MLflow configuration
mlflow:
  tracking_uri: "${MLFLOW_TRACKING_URI:http://localhost:5000}"
  experiment_name: "llama2_7b_experiments"
  log_model: true
  log_predictions: true
  register_model: true  # Register successful large model runs
  model_name: "llama2_7b_classifier"

# Evaluation settings
evaluation:
  compute_metrics: true
  metrics: ["accuracy", "f1", "precision", "recall"]
  save_predictions: true
  prediction_output_dir: "./predictions/llama2_7b"

# Environment
seed: 42
cuda_device: 0